#+TITLE: Neural Network in C
#+AUTHOR: Cristian Del Gobbo (pledged)
#+STARTUP: overview hideblocks indent
#+PROPERTY: header-args:C :main yes :includes <stdio.h> :results output

#+LATEX_HEADER: \usepackage{float}
#+CAPTION: "Dynamism of a Car" by Luigi Russolo (1913).
#+ATTR_LATEX: :float nil :placement [H] :width 0.4\textwidth
[[./Images/dynamism-of-a-car-luigi-russolo.jpg]]

* Credits: 
Credits for the NN written in C go to Nicolai Nielsen, as I followed his YouTube
video titled 'Coding a Neural Network from Scratch in C: No Libraries
Required.'
 
* Code Implementation
In this notebook, I want to compare a simple Neural Network written in =C= 
with a more "traditional" Neural Network implemented in =Python= using 
the =PyTorch= framework.

Both networks are designed to learn and predict the output of the =XOR= 
operation. The =XOR= function returns =true= (1) when exactly one of the 
two inputs is =true= and the other is =false=. It returns =false= (0) when 
both inputs are either =true= or both are =false=.

The Neural Network consists of two =input= nodes, three =hidden layers= 
with 2 =nodes= each, and one binary =output= (1 or 0). Initially, the network 
had only one =hidden layer=, but as an exercise, I added two additional =hidden layers=.

The code is somewhat repetitive, especially in the forward and backward passes. 
This suggests that it could be optimized by creating a potential =C= library with 
reusable functions to handle these common tasks, similar to how frameworks like =PyTorch= 
abstract these processes.

#+LATEX_HEADER: \usepackage{float}
#+CAPTION: "The Creation of Adam" by Michelangelo (1512).
#+ATTR_LATEX: :float nil :placement [H] :width 0.4\textwidth
[[./Images/Michelangelo_-_Creation_of_Adam_(cropped).jpg]]


** Neural Network in C
First of all, we need to define the structure of the =NN= and create the 
prototypes for the necessary functions. The network consists of two =inputs=, 
three =hidden layers= (with 3 =neurons= each, although the number of neurons 
may vary for testing purposes), and one =output=, which is the prediction for 
the =XOR= operation.

The most important functions include the =sigmoid=, which will serve as the activation 
function for the neurons in the network, and the =Derivative of Sigmoid= (=DSigmoid=), 
which is essential during backpropagation to update the weights. We need also a 
function to randomly initialize the weights and biases in the network (=init_weights=), 
ensuring that the learning process starts with non-zero values.

#+name: fp
#+begin_src C :cmdline -lm :main no 
  /*******************************************************/
  // nn.c: Create a basic NN that can learn XOR in C. 
  // (C) Cristian Del Gobbo Licence: GPLv3. 
  // Credits: Nicolai Nielsen
  /*******************************************************/
  #include <stdlib.h>
  #include <math.h>
  #include <time.h>
  #define NUM_INPUT 2
  #define NUM_HIDDEN_1 3
  #define NUM_HIDDEN_2 3
  #define NUM_HIDDEN_3 3
  #define NUM_OUTPUT 1
  #define NUM_TRAINING_SETS 4

  // Name: sigmoid
  // Purpose: Recreate the sigmoid function for logistic regression
  // Return: Double
  // Parameter: Double
  double sigmoid(double x);

  // Name: DSigmoid
  // Purpose: Take the derivative of the Sigmoid function
  // Return: Double
  // Parameter: Double
  double DSigmoid(double x);

  // Name: init_weights
  // Purpose: Initialize random weights.
  // Return: Double
  // Parameter: Nothing
  double init_weights();


  // Name: shuffle 
  // Purpose: Shuffle elements of an array.
  // Return:Nothing
  // Parameters: pointer to int array, size of the array
  void shuflle(int* array, size_t n);

  double ReLU(double x);

  double LeakyReLU(double x);

  double DLeakyReLU(double x);
  
  #+end_src

#+RESULTS: fp
  
The layers, weights, and biases can be represented through simple =arrays=, which store 
the activation values for the layers and the corresponding weights and biases for each layer.

The =training inputs= and =training outputs= represent the four cases for the =XOR= operation. 
This is a first approach, and in the future, I plan to experiment with different types of 
data to train the =Network=.

#+name: NN_Structure
#+begin_src C :noweb yes :results output
  // main function
  <<fp>>
  int main(){
    // Start timer
    clock_t start, end;
    start = clock();

    // Learning rate
    const double lr = 0.1f;

    // Define Layers, Bias, and weights 
    double hidden_layer_1[NUM_HIDDEN_1];
    double hidden_layer_2[NUM_HIDDEN_2];
    double hidden_layer_3[NUM_HIDDEN_3];
    double output_layer[NUM_OUTPUT];

    double hidden_layer_bias_1[NUM_HIDDEN_1];
    double hidden_layer_bias_2[NUM_HIDDEN_2];
    double hidden_layer_bias_3[NUM_HIDDEN_3];
    double output_layer_bias[NUM_OUTPUT];

    double hidden_weights_input_hidden_1[NUM_INPUT][NUM_HIDDEN_1];
    double hidden_weights_hidden_1_to_2[NUM_HIDDEN_1][NUM_HIDDEN_2];
    double hidden_weights_hidden_2_to_3[NUM_HIDDEN_2][NUM_HIDDEN_3];
    double output_weights[NUM_HIDDEN_3][NUM_OUTPUT];

    // Define training data
    double training_inputs[NUM_TRAINING_SETS][NUM_INPUT] = {{0.0f, 0.0f}, 
                                                            {1.0f, 0.0f}, 
                                                            {0.0f, 1.0f}, 
                                                            {1.0f, 1.0f}};

    double training_outputs[NUM_TRAINING_SETS][NUM_OUTPUT] = {{0.0f}, 
                                                              {1.0f}, 
                                                              {1.0f}, 
                                                              {0.0f}};
    #+end_src

#+RESULTS: NN_Structure


Here, we randomly initialize the =Weights= and =Biases= for the network. This ensures that the 
network starts without any built-in bias and can learn effectively. We initialize the weights 
for the connections between each layer, from input to hidden layers, and from hidden layers to 
the output layer, using the =init_weights= function. The biases are also initialized similarly for 
the output layer.

#+name: init_w_and_b
#+begin_src C :cmdline -lm :main no :noweb yes
  <<NN_structure>>
    // Input to Hidden layer
  for(int i = 0; i < NUM_INPUT; i++){
    for(int j = 0; j < NUM_HIDDEN_1; j++){
      hidden_weights_input_hidden_1[i][j] = init_weights();
    }
   }

  for(int i = 0; i < NUM_HIDDEN_1; i++){
    for(int j = 0; j < NUM_HIDDEN_2; j++){
      hidden_weights_hidden_1_to_2[i][j] = init_weights();
    }
   }

  for(int i = 0; i < NUM_HIDDEN_2; i++){
    for(int j = 0; j < NUM_HIDDEN_3; j++){
      hidden_weights_hidden_2_to_3[i][j] = init_weights();
    }
   }


  // Hidden to Output layer
  for(int i = 0; i < NUM_HIDDEN_3; i++){
    for(int j = 0; j < NUM_OUTPUT; j++){
      output_weights[i][j] = init_weights();
    }
   }

  // Initialize Biases
  for(int i = 0; i<NUM_OUTPUT; i++){
    output_layer_bias[i] = init_weights();
   }
#+end_src

#+RESULTS: init_w_and_b

 
Finally, =training=! In the forward pass, I used the randomly initialized weights and biases to compute 
the activation of each neuron. The activations are wrapped in the =Sigmoid= activation function, starting 
from the input layer, passing through the 3 hidden layers, and finally reaching the output.

#+name: forward
#+begin_src C :cmdline -lm :main no :noweb yes
  <<init_w_and_b>>

  // Shuffle Training set order
  int training_set_order[] = {0, 1, 2, 3};

  // Number of Epochs to train the model
  int number_of_epochs = 10000;

  // Train the neural network for n number of epochs
  for(int epoch = 0; epoch<number_of_epochs; epoch++){
    shuflle(training_set_order, NUM_TRAINING_SETS);
    for(int x = 0; x<NUM_TRAINING_SETS; x++){
      int i = training_set_order[x];

      // Forward pass
      // Compute Hidden Layer activation
      for(int j = 0; j < NUM_HIDDEN_1; j++){
        double activation = hidden_layer_bias_1[j];
        for(int k = 0; k < NUM_INPUT; k++){
          activation += training_inputs[i][k] * hidden_weights_input_hidden_1[k][j];
        }
        //hidden_layer_1[j] =  LeakyReLU(activation);
        hidden_layer_1[j] = sigmoid(activation);
      }

      for(int j = 0; j < NUM_HIDDEN_2; j++){
        double activation = hidden_layer_bias_2[j];
        for(int k = 0; k < NUM_HIDDEN_1; k++){
          //activation += hidden_layer_1[k] * hidden_weights_hidden_1_to_2[k][j]; //Changed
          activation += training_inputs[i][k] * hidden_weights_hidden_1_to_2[k][j];
        }
        //hidden_layer_2[j] =   LeakyReLU(activation);
        hidden_layer_2[j] = sigmoid(activation);
      }

      for(int j = 0; j < NUM_HIDDEN_3; j++){
        double activation = hidden_layer_bias_3[j];
        for(int k = 0; k < NUM_HIDDEN_2; k++){
          //activation += hidden_layer_2[k] * hidden_weights_hidden_2_to_3[k][j]; //Changed
          activation += training_inputs[i][k] * hidden_weights_hidden_2_to_3[k][j];
        }
        //hidden_layer_3[j] =   LeakyReLU(activation);
        hidden_layer_3[j] = sigmoid(activation);
      }


      // Compute Output Layer activation
      for(int j = 0; j < NUM_OUTPUT; j++){
        double activation = output_layer_bias[j];
        for(int k = 0; k < NUM_HIDDEN_3; k++){
          activation += hidden_layer_3[k] * output_weights[k][j];
        }
        output_layer[j] = sigmoid(activation);
      }
#+end_src

#+RESULTS: forward

In the backpropagation, the first step is to calculate the error for the =output layer= by comparing 
the predicted output to the actual =training outputs=. This error is then multiplied by the derivative 
of the =Sigmoid= function to compute the =delta= for the output layer.

Next, we compute the =delta= for the third hidden layer by propagating the error backward from the output 
layer. This is done by multiplying the error from the =output weights= by the =delta_output= and passing it 
through the derivative of the =Sigmoid= function for the activations in the third hidden layer.

The process is repeated for the second and first hidden layers, using the error from the previous layers 
and multiplying by the corresponding weights and the derivative of the =Sigmoid= function to compute the =delta= 
values for each layer.

#+name: back
#+begin_src C :cmdline -lm :main no :noweb yes
  <<forward>>
    // Backpropagation
    // Compute change in output weights
  double delta_output[NUM_OUTPUT];

  for(int j = 0; j<NUM_OUTPUT; j++){
    double error = (training_outputs[i][j] - output_layer[j]);
    delta_output[j] = error * DSigmoid(output_layer[j]);
   }

  // Compute change in hidden weights
  double delta_hidden_3[NUM_HIDDEN_3];
  for(int j = 0; j<NUM_HIDDEN_3; j++){
    double error = 0.0f;
    for(int k = 0; k<NUM_OUTPUT; k++){
      error += delta_output[k] * output_weights[j][k];
    }
    delta_hidden_3[j] = error * DSigmoid(hidden_layer_3[j]);
   }

  double delta_hidden_2[NUM_HIDDEN_2];
  for(int j = 0; j<NUM_HIDDEN_2; j++){
    double error = 0.0f;
    for(int k = 0; k<NUM_HIDDEN_3; k++){
      error += delta_hidden_3[k] * hidden_weights_hidden_2_to_3[j][k];
    }
    delta_hidden_2[j] = error * DSigmoid(hidden_layer_2[j]);
   }

  double delta_hidden_1[NUM_HIDDEN_1];
  for(int j = 0; j<NUM_HIDDEN_1; j++){
    double error = 0.0f;
    for(int k = 0; k<NUM_HIDDEN_2; k++){
      error += delta_hidden_2[k] * hidden_weights_hidden_1_to_2[j][k];
    }
    delta_hidden_1[j] = error * DSigmoid(hidden_layer_1[j]);
   }
#+end_src

#+RESULTS: back

After calculating the =delta= values during backpropagation, we apply the changes to the =weights= and =biases= for 
each layer.

Starting with the =output layer=, the =output weights= are adjusted by multiplying the corresponding =delta_output= 
values with the learning rate and adding them to the existing weights. The =output layer biases= are updated in a 
similar way.

Then, for each hidden layer, the same process is followed. The =delta_hidden= values are used to update the =weights= 
and =biases= between the layers, starting from the third hidden layer and going back to the first. This ensures that 
the network adjusts its weights and biases based on the error from the forward pass and optimizes the network's 
predictions through training.

#+name: ap
#+begin_src C :cmdline -lm :main no :noweb yes :results output
  <<back>>
  // Apply changes in output weights
  for(int j = 0; j<NUM_OUTPUT; j++){
    output_layer_bias[j] += delta_output[j] * lr;
    for(int k = 0; k<NUM_HIDDEN_3; k++){
      output_weights[k][j] += hidden_layer_3[k] * delta_output[j] * lr;
    }
   }

  for(int j = 0; j<NUM_HIDDEN_3; j++){
    hidden_layer_bias_3[j] += delta_hidden_3[j] * lr;
    for(int k = 0; k<NUM_HIDDEN_2; k++){
      hidden_weights_hidden_2_to_3[k][j] += hidden_layer_2[k] * delta_hidden_3[j] * lr;
    }
   }

  for(int j = 0; j<NUM_HIDDEN_2; j++){
    hidden_layer_bias_2[j] += delta_hidden_2[j] * lr;
    for(int k = 0; k<NUM_HIDDEN_1; k++){
      hidden_weights_hidden_1_to_2[k][j] += hidden_layer_1[k] * delta_hidden_2[j] * lr;
    }
   }

  // Apply changes in hidden weights
  for(int j = 0; j<NUM_HIDDEN_1; j++){
    hidden_layer_bias_1[j] += delta_hidden_1[j] * lr;
    for(int k = 0; k<NUM_INPUT; k++){
      hidden_weights_input_hidden_1[k][j] += training_inputs[i][k] * delta_hidden_1[j] * lr;
    }
   }
  printf("Epoch: %d Input: %g %g  Output: %g  Expected Output: %g \n", 
         epoch, training_inputs[i][0], training_inputs[i][1], 
         output_layer[0], training_outputs[i][0]);
  }

  }
#+end_src

#+RESULTS: ap

Almost at the end! In the =main= function, we stop the clock to measure how long the training process took and 
then print the result.

=Spoiler alert=: It's going to be much faster than the equivalent implementation in Python.
#+begin_src C :cmdline -lm :tangle nn.c :main no :noweb yes
  <<ap>>
  end = clock();
  double time_spent = ((double)(end-start)/CLOCKS_PER_SEC);

  printf("\nTime taken to run the NN in C: %f seconds\n", time_spent);
  return 0;
  }

  // Function declarations
  double max(double x, double y){
    if(x>y)
      return x;
    else
      return y;
  }

  double sigmoid(double x){
    return 1 / (1 + exp(-x));
  }

  double DSigmoid(double x){
    return x * (1-x);
  }

  double ReLU(double x){
    return x > 0 ? x : 0;
  }

  double LeakyReLU(double x){
    return x > 0 ? x : 0.01 * x;
  }

  double DLeakyReLU(double x) {
    return x > 0 ? 1 : 0.01; 
  }

  double init_weights(){
    return ((double)rand()) / ((double)RAND_MAX);
  }

  void shuflle(int* array, size_t n){
    if(n>1){
      size_t i;
      for(i=0; i<n-1; i++){
        size_t j = i + rand() / (RAND_MAX / (n-i) + 1);
        int t = array[j];
        array[j] = array[i];
        array[i] = t;
      }
    }
  }
#+end_src

#+RESULTS: func

#+RESULTS:

** Neural Network in Python (PyTorch)
In the following code block is the 'Classic' =PyTorch= implementation of the same neural network. 
It’s definitely much shorter, but how does it compare in terms of training time and efficiency? 

#+begin_src python :results output :tangle nn.py
  import time
  import torch
  import torch.nn as nn
  import torch.optim as optim

  # Simple NN with PyTorch
  class SimpleNN(nn.Module):
      def __init__(self):
          super(SimpleNN, self).__init__()
          self.hidden_1 = nn.Linear(2, 2)
          self.hidden_2 = nn.Linear(2, 2)
          self.hidden_3 = nn.Linear(2, 2)
          self.output = nn.Linear(2, 1)
          self.relu = nn.ReLU()
          self.sigmoid = nn.Sigmoid()

      def forward(self, x):
          x = self.relu(self.hidden_1(x))
          x = self.relu(self.hidden_2(x))
          x = self.relu(self.hidden_3(x))
          x = self.sigmoid(self.output(x))
          return x

  # Define the model
  model = SimpleNN()
  criterion = nn.MSELoss()
  optimizer = optim.SGD(model.parameters(), lr=0.1)

  # Training data
  inputs = torch.tensor([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])
  #inputs = inputs.repeat(1000, 1)
  targets = torch.tensor([[0.0], [1.0], [1.0], [0.0]])
  #targets = targets.repeat(1000, 1)

  # Measure the execution time
  start_time = time.time()

  # Training the network
  epochs = 10000
  for epoch in range(epochs):
      for i in range(inputs.size(0)):
          optimizer.zero_grad()
          y_pred = model(inputs[i].unsqueeze(0))
          loss = criterion(y_pred, targets[i].unsqueeze(0))
          loss.backward()
          optimizer.step()
          print(f"Epoch: {epoch},  Input: {inputs[i]}, Output: {y_pred.item()},  Expected Output: {targets[i].item()}")

  # End time
  end_time = time.time()

  print(f"\nTime taken to run the NN in Python (PyTorch): {end_time - start_time:.6f} seconds")

#+end_src

#+RESULTS:

** Neural Network in Python (From Scratch)
Before comparing the speed and efficiency of the =C= and =PyTorch= networks, I first wanted to create 
a similar implementation from scratch using plain =Python=, without the abstraction provided by =PyTorch=. 
This way, I can assess if =PyTorch='s abstraction slows it down compared to a raw Python implementation.

#+begin_src python :results output :tangle nns.py
import numpy as np
import random
import time

num_input = 2
num_hidden_1 = 3
num_hidden_2 = 3
num_hidden_3 = 3
num_output = 1
num_training_sets = 4

def sigmoid(x):
    return 1/(1+np.exp(-x))

def DSigmoid(x):
    return x * (1-x)

def init_weights():
    return random.random()

start_time = time.time()

lr = 0.1

hidden_layer_1 = np.zeros(num_hidden_1)
hidden_layer_2 = np.zeros(num_hidden_2)
hidden_layer_3 = np.zeros(num_hidden_3)
output_layer = np.zeros(num_output)

hidden_layer_bias_1 = np.random.rand(num_hidden_1)
hidden_layer_bias_2 = np.random.rand(num_hidden_2)
hidden_layer_bias_3 = np.random.rand(num_hidden_3)
output_layer_bias = np.random.rand(num_output)

hidden_weights_input_hidden_1 = np.random.rand(num_input, num_hidden_1)
hidden_weights_hidden_1_to_2 = np.random.rand(num_hidden_1, num_hidden_2)
hidden_weights_hidden_2_to_3 = np.random.rand(num_hidden_2, num_hidden_3)
output_weights = np.random.rand(num_hidden_3, num_output)

training_inputs = [[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]
training_outputs = [[0.0], [1.0], [1.0], [0.0]]

training_set_order = [0, 1, 2, 3]

number_of_epochs = 10000

# Start training 
for epoch in range(number_of_epochs):
    random.shuffle(training_set_order)
    for x in range(num_training_sets):
        i = training_set_order[x]

        for j in range(num_hidden_1):
            activation = hidden_layer_bias_1[j]
            for k in range(num_input):
                activation += training_inputs[i][k] * hidden_weights_input_hidden_1[k][j]
            hidden_layer_1[j] = sigmoid(activation)

        for j in range(num_hidden_2):
            activation = hidden_layer_bias_2[j]
            for k in range(num_hidden_1):
                activation += hidden_layer_1[k] * hidden_weights_hidden_1_to_2[k][j]
            hidden_layer_2[j] = sigmoid(activation)

        for j in range(num_hidden_3):
            activation = hidden_layer_bias_3[j]
            for k in range(num_hidden_2):
                activation += hidden_layer_2[k] * hidden_weights_hidden_2_to_3[k][j]
            hidden_layer_3[j] = sigmoid(activation)

        for j in range(num_output):
            activation = output_layer_bias[j]
            for k in range(num_hidden_3):
                activation += hidden_layer_3[k] * output_weights[k][j]
            output_layer[j] = sigmoid(activation)

        # Backpropagation
        delta_output = np.zeros(num_output)
        for j in range(num_output):
            error = (training_outputs[i][j] - output_layer[j])
            delta_output[j] = error * DSigmoid(output_layer[j])

        delta_hidden_3 = np.zeros(num_hidden_3)
        for j in range(num_hidden_3):
            error = 0.0
            for k in range(num_output):
                error += delta_output[k] * output_weights[j][k]
            delta_hidden_3[j] = error * DSigmoid(hidden_layer_3[j])

        delta_hidden_2 = np.zeros(num_hidden_2)
        for j in range(num_hidden_2):
            error = 0.0
            for k in range(num_hidden_3):
                error += delta_hidden_3[k] * hidden_weights_hidden_2_to_3[k][j]
            delta_hidden_2[j] = error * DSigmoid(hidden_layer_2[j])

        delta_hidden_1 = np.zeros(num_hidden_1)
        for j in range(num_hidden_1):
            error = 0.0
            for k in range(num_hidden_2):
                error += delta_hidden_2[k] * hidden_weights_hidden_1_to_2[k][j]
            delta_hidden_1[j] = error * DSigmoid(hidden_layer_1[j])

        # Apply changes 
        for j in range(num_output):
            output_layer_bias[j] += delta_output[j] * lr
            for k in range(num_hidden_3):
                output_weights[k][j] += hidden_layer_3[k] * delta_output[j] * lr

        for j in range(num_hidden_3):
            hidden_layer_bias_3[j] += delta_hidden_3[j] * lr
            for k in range(num_hidden_2):
                hidden_weights_hidden_2_to_3[k][j] += hidden_layer_2[k] * delta_hidden_3[j] * lr

        for j in range(num_hidden_2):
            hidden_layer_bias_2[j] += delta_hidden_2[j] * lr
            for k in range(num_hidden_1):
                hidden_weights_hidden_1_to_2[k][j] += hidden_layer_1[k] * delta_hidden_2[j] * lr

        for j in range(num_hidden_1):
            hidden_layer_bias_1[j] += delta_hidden_1[j] * lr
            for k in range(num_input):
                hidden_weights_input_hidden_1[k][j] += training_inputs[i][k] * delta_hidden_1[j] * lr

        print(f"Epoch: {epoch}, Input: {training_inputs[i][0]} {training_inputs[i][1]}, Output: {output_layer[0]}, Expected Output: {training_outputs[i][0]}")

end_time = time.time()
print(f"\nTime taken to run the NN in Python (From Scratch): {end_time - start_time:.6f} seconds")

#+end_src

#+RESULTS:

* Expected Results
Not surprisingly, =C= is significantly faster than =Python= (Both PyTorch and Python from scratch) when comparing the runtime 
for the neural network implementations above. Below is a screenshot of the outputs 
from the =C= and =Python= codes, both executed with 10000 epochs:

#+LATEX_HEADER: \usepackage{float}
#+CAPTION: Final Comparison.
#+ATTR_LATEX: :float nil :placement [H] :width 0.4\textwidth
[[./Images/comparison.png]]

However, I want to highlight a few key points:

- Implementing the neural network in =PyTorch= is much easier due to the high level 
  of abstraction provided by the framework.

- The Python From Scratch version outperformed the PyTorch version, it was more than 
  10 seconds faster. 

- Writing the neural network code in =C= gives a deeper understanding of each step of 
  the process, making it a valuable learning experience. I would recommend this approach 
  for beginners to fully grasp the underlying mechanics.

- It was a fun exercise.


"il naufragar m'è dolce in questo mare"
67 114 105 115 116 105 97 110

